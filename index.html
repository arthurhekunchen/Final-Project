<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>“What Makes Great Wine Great?” —Prediction of wine’s quality based on data science techniques</title>

<script src="site_libs/header-attrs-2.16/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>









<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark the anchor link active (and if it's in a dropdown, also mark that active)
  var dropdown = menuAnchor.closest('li.dropdown');
  if (window.bootstrap) { // Bootstrap 4+
    menuAnchor.addClass('active');
    dropdown.find('> .dropdown-toggle').addClass('active');
  } else { // Bootstrap 3
    menuAnchor.parent().addClass('active');
    dropdown.addClass('active');
  }

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->




</head>

<body>


<div class="container-fluid main-container">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-bs-toggle="collapse" data-target="#navbar" data-bs-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">My Website</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">“What Makes Great Wine Great?” —Prediction
of wine’s quality based on data science techniques</h1>

</div>


<p>Introduction</p>
<ol style="list-style-type: decimal">
<li><p>Dataset: The data is about red wine samples (vinho verde) from
Portugal. It was collected from May 2004 to February 2007 using only
protected designation of origin samples that were tested at the official
certification entity.</p></li>
<li><p>Question: The objective is to determine the most effective method
for determining the wine’s quality, or in other words, explore the
variables that may impact the quality of the wine.</p></li>
</ol>
<p>EDA &amp; Modelling Results</p>
<ol style="list-style-type: decimal">
<li>Load Needed Libaries</li>
</ol>
<pre class="r"><code>library(ggplot2) # Plotting 
library(GGally) # ggpairs plot</code></pre>
<pre><code>## Registered S3 method overwritten by &#39;GGally&#39;:
##   method from   
##   +.gg   ggplot2</code></pre>
<pre class="r"><code>library(caret) # Showing Confusion Matrix Data (among many other useful functions)</code></pre>
<pre><code>## Loading required package: lattice</code></pre>
<pre class="r"><code>library(purrr) # Organizing</code></pre>
<pre><code>## 
## Attaching package: &#39;purrr&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:caret&#39;:
## 
##     lift</code></pre>
<pre class="r"><code>library(tidyr) # Organize/tidy data
library(reshape) # Melt data for plotting</code></pre>
<pre><code>## 
## Attaching package: &#39;reshape&#39;</code></pre>
<pre><code>## The following objects are masked from &#39;package:tidyr&#39;:
## 
##     expand, smiths</code></pre>
<pre class="r"><code>library(knitr) #kable(head(wineDf)) </code></pre>
<ol start="2" style="list-style-type: decimal">
<li>Exploratory Data Analysis: Exploratory data analysis will be
performed in this part to see how the data variables look like and how
quality relates to the other variables individually.</li>
</ol>
<pre class="r"><code>## Read in the Data
wineDf = read.csv(&quot;winequality-red.csv&quot;, header=T) # Load the data
wineDf.orig = wineDf # Save off original copy before making any alterations
## Change quality to categorical
wineDf$quality = as.ordered(wineDf$quality) # Change quality to (ordered) factor
## Split wine quality into good wine or bad wine for later
wineDfBinaryQuality = wineDf.orig
wineDfBinaryQuality$quality = as.factor(ifelse(wineDf.orig$quality&gt;5.5,&quot;Good&quot;,&quot;Bad&quot;))
## Visual Look at the Numeric/Continuous Variables in Data Set
wineDf %&gt;%
  keep(is.numeric) %&gt;% 
  gather() %&gt;% 
  ggplot(aes(value,fill=key)) +
  facet_wrap(~ key, scales = &quot;free&quot;) +
  geom_histogram(bins=sqrt(nrow(wineDf))) +
  theme(legend.position=&quot;none&quot;) </code></pre>
<p><img src="Index_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<pre class="r"><code>### Log transformation
wineDf %&gt;%
  keep(is.numeric) %&gt;% 
  gather() %&gt;% 
  ggplot(aes(value,fill=key)) +
  facet_wrap(~ key, scales = &quot;free&quot;) +
  geom_histogram(bins=sqrt(nrow(wineDf))) +
  theme(legend.position=&quot;none&quot;) +
  scale_x_continuous(trans=&#39;log10&#39;)</code></pre>
<pre><code>## Warning: Transformation introduced infinite values in continuous x-axis</code></pre>
<pre><code>## Warning: Removed 132 rows containing non-finite values (stat_bin).</code></pre>
<p><img src="Index_files/figure-html/unnamed-chunk-2-2.png" width="672" /></p>
<p>Interpretation: The first figure shows that density and pH appear to
be symmetric, with no heavy tails. This indicates that the distribution
appears to be normal distribution. Volatile acidity has a reasonably
typical distribution as well. Looking at the remaining variables, one
can observe that a number of distributions, such as chlorides, fixed
acidity, residual sugar, and sulphates, are skewed (having a right
tail). This suggests that the majority of the wines in these
distributions have lower end values, with a few exceptions having
somewhat higher end values. Looking at residual sugar, for example, the
majority of the numbers lie between 0 and 4 grams of residual sugar per
decimeter cubed of wine; nevertheless, there are a few values that are
higher than 4 and even some that are lower. Here, log transformation is
applied to improve the skewed distribution. By setting the x axis to a
log scale, we can demonstrate this impact. Below are the identical
graphs, but with the x axis displayed on a log scale. We then observe in
the second Figure that the previously skewed distributions appear more
visually normal. This illustrates the results of applying a log
transformation to a (right tail) skewed distribution. Now that the
numerical/continuous data has been examined, let’s use a bar plot to
examine quality, the only variable in the data that resembles a
categorical one.</p>
<pre class="r"><code>## Visualization of Wine Quality
ggplot(wineDf, aes(x=quality, fill = quality)) +
  geom_bar(stat=&quot;count&quot;) +
  geom_text(position = &quot;stack&quot;, stat=&#39;count&#39;,aes(label=..count..), vjust = -0.5)+
  labs(y=&quot;Num of Observations&quot;, x=&quot;Wine Quality&quot;) +
  theme(legend.position=&quot;none&quot;)</code></pre>
<p><img src="Index_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>Interpretation: The figure illustrates how quality is not evenly
distributed from 0 to 10. The majority of numbers fall between 5 and 6.
In other words, there are significantly more average wines than
extremely good or subpar wines. Because of this, it could be more
challenging to define what makes a good or bad wine. To help with this,
one may simply divide the wines into two categories: good wine quality
and low wine quality. It would be logical to divide the wines into bad
and good wines by 5 or below and 6 or above if one wants to create a
binary wine quality variable that is consistent across all wines because
5 and 6 have a pretty even number of wines and have the majority of the
wines overall.</p>
<pre class="r"><code>## Visual Look at the Data Set with Pairs Plot Colored by Good/Bad Wine Quality
ggpairs(wineDfBinaryQuality, 
        aes(alpha=0.6, color = quality),
        upper = list(continuous = wrap(&quot;cor&quot;, size = 2)),
        diag = list(continuous = &quot;barDiag&quot;),
        lower = list(continuous = &quot;smooth&quot;))</code></pre>
<pre><code>## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.</code></pre>
<p><img src="Index_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>Interpretation: One can more clearly distinguish between the two
groups when the data is colored by good vs. bad wine. For instance, the
alcohol and quality plots in the bottom right corner demonstrate that
while some poor wines have low alcohol content (in a skewed
distribution), superior wines have a more even distribution and
generally have greater alcohol contents. Additionally, one can observe
from the scatter plots and trend lines at the bottom left that, for each
set of factors, terrible and good wines generally appear to follow a
similar trend. We can observe how each numerical variable is distributed
in relation to good vs. bad quality from the box plots for bad vs. good
quality on the right. As was already said, fine wines typically have
more alcohol. In general, good wines also tend to have more citric acid
and sulfates. Bad wines typically have slightly greater densities and
more volatile acidity. Based on the box plots, it is difficult to
visually distinguish between good and terrible wines using the other
numerical factors.</p>
<ol start="3" style="list-style-type: decimal">
<li>Model implementation</li>
</ol>
<pre class="r"><code>## Spliting the data into training set and test set
df &lt;- read.csv(&quot;winequality-red.csv&quot;)
df$quality &lt;- as.factor(df$quality)
set.seed(123)
index &lt;- sample(1:nrow(df),size = 0.8*nrow(df))
train &lt;- df[index,]
test &lt;- df[-index,]
## Linear regression
df_lm &lt;- read.csv(&quot;winequality-red.csv&quot;)
train_lm &lt;- df_lm[index,]
test_lm &lt;- df_lm[-index,]
lm_model &lt;- glm(quality ~., data = train_lm)
lm_prediction &lt;- predict(lm_model, test_lm)
lm_prediction &lt;- round(lm_prediction, digits = 0)
lm_prediction &lt;- as.factor(lm_prediction)
test_lm$quality &lt;- as.factor(test_lm$quality)
lm_matrix &lt;- confusionMatrix(lm_prediction, test_lm$quality)</code></pre>
<pre><code>## Warning in confusionMatrix.default(lm_prediction, test_lm$quality): Levels are
## not in the same order for reference and data. Refactoring data to match.</code></pre>
<pre class="r"><code>lm_matrix</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction   3   4   5   6   7   8
##          3   0   0   0   0   0   0
##          4   0   0   0   0   0   0
##          5   2   8 107  43   0   0
##          6   0   1  42  81  16   2
##          7   0   0   0   7  11   0
##          8   0   0   0   0   0   0
## 
## Overall Statistics
##                                           
##                Accuracy : 0.6219          
##                  95% CI : (0.5663, 0.6752)
##     No Information Rate : 0.4656          
##     P-Value [Acc &gt; NIR] : 1.409e-08       
##                                           
##                   Kappa : 0.3489          
##                                           
##  Mcnemar&#39;s Test P-Value : NA              
## 
## Statistics by Class:
## 
##                      Class: 3 Class: 4 Class: 5 Class: 6 Class: 7 Class: 8
## Sensitivity           0.00000  0.00000   0.7181   0.6183  0.40741  0.00000
## Specificity           1.00000  1.00000   0.6901   0.6772  0.97611  1.00000
## Pos Pred Value            NaN      NaN   0.6687   0.5704  0.61111      NaN
## Neg Pred Value        0.99375  0.97188   0.7375   0.7191  0.94702  0.99375
## Prevalence            0.00625  0.02813   0.4656   0.4094  0.08438  0.00625
## Detection Rate        0.00000  0.00000   0.3344   0.2531  0.03438  0.00000
## Detection Prevalence  0.00000  0.00000   0.5000   0.4437  0.05625  0.00000
## Balanced Accuracy     0.50000  0.50000   0.7041   0.6478  0.69176  0.50000</code></pre>
<pre class="r"><code># logistic regression
df_lr &lt;- read.csv(&quot;winequality-red.csv&quot;)
df_lr$quality &lt;- ifelse(df_lr$quality&gt;5,1,0)
df_lr$quality &lt;- as.factor(df_lr$quality)
train_lr &lt;- df_lr[index,]
test_lr &lt;- df_lr[-index,]
lr_model &lt;- glm(quality ~., data = train_lr, family = &quot;binomial&quot;)
lr_prediction &lt;- predict(lr_model, test_lr, type = &quot;response&quot;)
lr_prediction &lt;- round(lr_prediction, digits = 0)
lr_prediction &lt;- as.factor(lr_prediction)
lr_matrix &lt;- confusionMatrix(lr_prediction, test_lr$quality)
lr_matrix</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction   0   1
##          0 124  48
##          1  36 112
##                                           
##                Accuracy : 0.7375          
##                  95% CI : (0.6857, 0.7849)
##     No Information Rate : 0.5             
##     P-Value [Acc &gt; NIR] : &lt;2e-16          
##                                           
##                   Kappa : 0.475           
##                                           
##  Mcnemar&#39;s Test P-Value : 0.2301          
##                                           
##             Sensitivity : 0.7750          
##             Specificity : 0.7000          
##          Pos Pred Value : 0.7209          
##          Neg Pred Value : 0.7568          
##              Prevalence : 0.5000          
##          Detection Rate : 0.3875          
##    Detection Prevalence : 0.5375          
##       Balanced Accuracy : 0.7375          
##                                           
##        &#39;Positive&#39; Class : 0               
## </code></pre>
<p>Interpretation: The linear model’s performance is ordinary, according
to the evaluation metrics we use here. Only a 0.6219 accuracy out of
1.0, with CI of (0.5663, 0.6752). The kappa coefficient was calculated
as 0.3489, which is a considerably fair result. The performance of
logstic model does improve referring to the evaluation metrics here,
compared to linear regression model fitted earlier. (0.7375 accuracy out
of 1.0, with CI of (0.6857, 0.7489)) However, it’s much easier for the
model to guess right when there are only 2 possible outcomes in this
case.</p>
<pre class="r"><code># simplest random forest
library(randomForest)</code></pre>
<pre><code>## randomForest 4.7-1.1</code></pre>
<pre><code>## Type rfNews() to see new features/changes/bug fixes.</code></pre>
<pre><code>## 
## Attaching package: &#39;randomForest&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:ggplot2&#39;:
## 
##     margin</code></pre>
<pre class="r"><code>rf_model &lt;- randomForest(quality ~., data = train)
rf_prediction &lt;- predict(rf_model, test)
library(caret)
rf_matrix &lt;- confusionMatrix(rf_prediction, test$quality)
rf_matrix</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction   3   4   5   6   7   8
##          3   0   0   0   0   0   0
##          4   0   0   0   0   0   0
##          5   1   7 120  30   0   0
##          6   1   2  27  95  14   1
##          7   0   0   2   6  13   0
##          8   0   0   0   0   0   1
## 
## Overall Statistics
##                                           
##                Accuracy : 0.7156          
##                  95% CI : (0.6628, 0.7644)
##     No Information Rate : 0.4656          
##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16       
##                                           
##                   Kappa : 0.5143          
##                                           
##  Mcnemar&#39;s Test P-Value : NA              
## 
## Statistics by Class:
## 
##                      Class: 3 Class: 4 Class: 5 Class: 6 Class: 7 Class: 8
## Sensitivity           0.00000  0.00000   0.8054   0.7252  0.48148 0.500000
## Specificity           1.00000  1.00000   0.7778   0.7619  0.97270 1.000000
## Pos Pred Value            NaN      NaN   0.7595   0.6786  0.61905 1.000000
## Neg Pred Value        0.99375  0.97188   0.8210   0.8000  0.95318 0.996865
## Prevalence            0.00625  0.02813   0.4656   0.4094  0.08438 0.006250
## Detection Rate        0.00000  0.00000   0.3750   0.2969  0.04063 0.003125
## Detection Prevalence  0.00000  0.00000   0.4938   0.4375  0.06563 0.003125
## Balanced Accuracy     0.50000  0.50000   0.7916   0.7435  0.72709 0.750000</code></pre>
<pre class="r"><code># More complicated random forest using caret and ranger packages (10-fold cross-validation)
library(ranger)</code></pre>
<pre><code>## 
## Attaching package: &#39;ranger&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:randomForest&#39;:
## 
##     importance</code></pre>
<pre class="r"><code>caret_rf_model &lt;- train(
  quality ~ .,
  tuneLength = 10,
  data = train, method = &quot;ranger&quot;,
  trControl = trainControl(method = &quot;cv&quot;, number = 10, verboseIter = FALSE)
)
caret_rf_predict &lt;- predict(caret_rf_model, test)
caret_rf_matrix &lt;- confusionMatrix(caret_rf_predict, test$quality)
caret_rf_matrix</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction   3   4   5   6   7   8
##          3   0   0   0   0   0   0
##          4   0   0   0   0   0   0
##          5   1   6 120  31   0   0
##          6   1   3  27  93  12   1
##          7   0   0   2   7  15   0
##          8   0   0   0   0   0   1
## 
## Overall Statistics
##                                           
##                Accuracy : 0.7156          
##                  95% CI : (0.6628, 0.7644)
##     No Information Rate : 0.4656          
##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16       
##                                           
##                   Kappa : 0.5168          
##                                           
##  Mcnemar&#39;s Test P-Value : NA              
## 
## Statistics by Class:
## 
##                      Class: 3 Class: 4 Class: 5 Class: 6 Class: 7 Class: 8
## Sensitivity           0.00000  0.00000   0.8054   0.7099  0.55556 0.500000
## Specificity           1.00000  1.00000   0.7778   0.7672  0.96928 1.000000
## Pos Pred Value            NaN      NaN   0.7595   0.6788  0.62500 1.000000
## Neg Pred Value        0.99375  0.97188   0.8210   0.7923  0.95946 0.996865
## Prevalence            0.00625  0.02813   0.4656   0.4094  0.08438 0.006250
## Detection Rate        0.00000  0.00000   0.3750   0.2906  0.04688 0.003125
## Detection Prevalence  0.00000  0.00000   0.4938   0.4281  0.07500 0.003125
## Balanced Accuracy     0.50000  0.50000   0.7916   0.7386  0.76242 0.750000</code></pre>
<pre class="r"><code>## look at the importance of variables
varImp(rf_model) %&gt;% kable</code></pre>
<table>
<thead>
<tr class="header">
<th align="left"></th>
<th align="right">Overall</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">fixed.acidity</td>
<td align="right">64.12203</td>
</tr>
<tr class="even">
<td align="left">volatile.acidity</td>
<td align="right">83.16457</td>
</tr>
<tr class="odd">
<td align="left">citric.acid</td>
<td align="right">62.74116</td>
</tr>
<tr class="even">
<td align="left">residual.sugar</td>
<td align="right">59.93305</td>
</tr>
<tr class="odd">
<td align="left">chlorides</td>
<td align="right">67.63975</td>
</tr>
<tr class="even">
<td align="left">free.sulfur.dioxide</td>
<td align="right">55.65824</td>
</tr>
<tr class="odd">
<td align="left">total.sulfur.dioxide</td>
<td align="right">87.67934</td>
</tr>
<tr class="even">
<td align="left">density</td>
<td align="right">76.77900</td>
</tr>
<tr class="odd">
<td align="left">pH</td>
<td align="right">62.75414</td>
</tr>
<tr class="even">
<td align="left">sulphates</td>
<td align="right">90.97401</td>
</tr>
<tr class="odd">
<td align="left">alcohol</td>
<td align="right">119.96149</td>
</tr>
</tbody>
</table>
<pre class="r"><code>varImpPlot(rf_model)</code></pre>
<p><img src="Index_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<p>Interpretation: The variable importance plot in Figure 8 provides a
list of the most significant variables in descending order by a mean
decrease in Gini. The top variables contribute more to the model than
the bottom ones and also have high predictive power in classifying
default and non-default customers. a) In this case, alcohol has the
largest mean decrease in Gini compared to other variables, which
demonstrates that it has high predictive power in predicting our target
variable – wine quality. This is kind of reasonable since A wine with a
higher alcohol content will have a fuller, richer body, while a
lower-level alcohol wine will taste lighter and more delicate on the
palate (Masterclass Staff, 2020). b) Sulfites are a group of chemical
compounds found naturally in a variety of foods and beverages. Research
shows that a small percentage of the population is even sensitive to
sulfites and may experience side effects like headaches, hives,
swelling, stomach pain, and diarrhea. That may be one of the reasons why
it’s the second most important of our all variables. c) Besides, Total
sulfur dioxide, Volatile acidity and Density also show high importance
in our analysis, which may contribute to the realistic wine
manufacturing process.</p>
<pre class="r"><code># SVM
library(e1071)

tune_out &lt;- 
    tune.svm(x = train[, -12], y = train[, 12], 
             type = &quot;C-classification&quot;, cost = c(0.1, 1, 10, 100), 
             gamma = c(0.1, 1, 10), coef0 = c(0.1, 1, 10))

svm_model &lt;- svm(quality~ ., data = train, type = &quot;C-classification&quot;, 
                 cost = tune_out$best.parameters$cost, 
                 gamma = tune_out$best.parameters$gamma, 
                 coef0 = tune_out$best.parameters$coef0)

svm_predict &lt;- predict(svm_model, test)
svm_predict &lt;- as.factor(svm_predict)
svm_matrix &lt;- confusionMatrix(svm_predict, test$quality)
svm_matrix</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction   3   4   5   6   7   8
##          3   0   0   0   0   0   0
##          4   0   0   0   0   0   0
##          5   1   7 116  38   2   0
##          6   1   2  32  90  17   0
##          7   0   0   1   3   8   2
##          8   0   0   0   0   0   0
## 
## Overall Statistics
##                                           
##                Accuracy : 0.6688          
##                  95% CI : (0.6142, 0.7201)
##     No Information Rate : 0.4656          
##     P-Value [Acc &gt; NIR] : 1.924e-13       
##                                           
##                   Kappa : 0.4249          
##                                           
##  Mcnemar&#39;s Test P-Value : NA              
## 
## Statistics by Class:
## 
##                      Class: 3 Class: 4 Class: 5 Class: 6 Class: 7 Class: 8
## Sensitivity           0.00000  0.00000   0.7785   0.6870  0.29630  0.00000
## Specificity           1.00000  1.00000   0.7193   0.7249  0.97952  1.00000
## Pos Pred Value            NaN      NaN   0.7073   0.6338  0.57143      NaN
## Neg Pred Value        0.99375  0.97188   0.7885   0.7697  0.93791  0.99375
## Prevalence            0.00625  0.02813   0.4656   0.4094  0.08438  0.00625
## Detection Rate        0.00000  0.00000   0.3625   0.2812  0.02500  0.00000
## Detection Prevalence  0.00000  0.00000   0.5125   0.4437  0.04375  0.00000
## Balanced Accuracy     0.50000  0.50000   0.7489   0.7059  0.63791  0.50000</code></pre>
<p>The accuracy of SVM is 0.6688, which indicates that the SVM model
correctly predicts the value for quality more than 66% of the time. It’s
just slightly worse than the random forest model, and much better than
linear regression.</p>
<p>Conclusion</p>
<p>o I was able to construct a model that can help industry producers,
distributors, and sellers forecast the quality of red wine products and
better grasp each key and up-to-date characteristic by analyzing the
physicochemical test samples data of red wines from the north of
Portugal.</p>
<p>o Regarding our primary question, all features in our dataset have
shown an effect on the quality of wine. The major findings are that
alcohol level has had a major effect in determining the quality of wine.
However, the increase in alcohol level has also been viewed as a feature
of good wine but, it should not increase to an amount where the wine
will be categorized as hard liquor.</p>
<p>o I also discovered that Model #2 — Random Forest outperformed others
with evaluation metrics in Table 3 below. The model indicates that 5 of
the features were the most influential: alcohol, sulphates, Total sulfur
dioxide, Volatile acidity and Density. High-quality wines appear to have
lower volatile acidity, greater alcohol, and medium-high sulphate
readings.</p>
<p>References</p>
<p>[1] Data Source: <a
href="https://archive.ics.uci.edu/ml/datasets/wine+quality"
class="uri">https://archive.ics.uci.edu/ml/datasets/wine+quality</a></p>
<p>[2] James, G., Witten, D., Hastie, T., and Tibshirani, R. (2013) An
Introduction to Statistical Learning with applications in R,
www.StatLearning.com, Springer-Verlag, New York</p>
<p>[3] P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis.
Modeling wine preferences by data mining from physicochemical
properties. In Decision Support Systems, Elsevier, 47(4):547-553,
2009.</p>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
