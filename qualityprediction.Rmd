---
title: "qualityprodiction"
author: "Arthur Chen"
date: "2022-12-15"
output: github_document
---

1. Load Needed Libaries
```{r}
library(ggplot2) # Plotting 
library(GGally) # ggpairs plot
library(caret) # Showing Confusion Matrix Data (among many other useful functions)
library(purrr) # Organizing
library(tidyr) # Organize/tidy data
library(reshape) # Melt data for plotting
library(knitr) #kable(head(wineDf)) 
```

2. Exploratory Data Analysis: Exploratory data analysis will be performed in this part to see how the data variables look like and how quality relates to the other variables individually.
```{r}
## Read in the Data
wineDf = read.csv("winequality-red.csv", header=T) # Load the data
wineDf.orig = wineDf # Save off original copy before making any alterations
## Change quality to categorical
wineDf$quality = as.ordered(wineDf$quality) # Change quality to (ordered) factor
## Split wine quality into good wine or bad wine for later
wineDfBinaryQuality = wineDf.orig
wineDfBinaryQuality$quality = as.factor(ifelse(wineDf.orig$quality>5.5,"Good","Bad"))
## Visual Look at the Numeric/Continuous Variables in Data Set
wineDf %>%
  keep(is.numeric) %>% 
  gather() %>% 
  ggplot(aes(value,fill=key)) +
  facet_wrap(~ key, scales = "free") +
  geom_histogram(bins=sqrt(nrow(wineDf))) +
  theme(legend.position="none") 
### Log transformation
wineDf %>%
  keep(is.numeric) %>% 
  gather() %>% 
  ggplot(aes(value,fill=key)) +
  facet_wrap(~ key, scales = "free") +
  geom_histogram(bins=sqrt(nrow(wineDf))) +
  theme(legend.position="none") +
  scale_x_continuous(trans='log10')
```

Interpretation: The first figure shows that density and pH appear to be symmetric, with no heavy tails. This indicates that the distribution appears to be normal distribution. Volatile acidity has a reasonably typical distribution as well. 
Looking at the remaining variables, one can observe that a number of distributions, such as chlorides, fixed acidity, residual sugar, and sulphates, are skewed (having a right tail). This suggests that the majority of the wines in these distributions have lower end values, with a few exceptions having somewhat higher end values. Looking at residual sugar, for example, the majority of the numbers lie between 0 and 4 grams of residual sugar per decimeter cubed of wine; nevertheless, there are a few values that are higher than 4 and even some that are lower.
Here, log transformation is applied to improve the skewed distribution. By setting the x axis to a log scale, we can demonstrate this impact. Below are the identical graphs, but with the x axis displayed on a log scale.
We then observe in the second Figure that the previously skewed distributions appear more visually normal. This illustrates the results of applying a log transformation to a (right tail) skewed distribution.
Now that the numerical/continuous data has been examined, let's use a bar plot to examine quality, the only variable in the data that resembles a categorical one.

```{r}
## Visualization of Wine Quality
ggplot(wineDf, aes(x=quality, fill = quality)) +
  geom_bar(stat="count") +
  geom_text(position = "stack", stat='count',aes(label=..count..), vjust = -0.5)+
  labs(y="Num of Observations", x="Wine Quality") +
  theme(legend.position="none")
```

Interpretation: The figure illustrates how quality is not evenly distributed from 0 to 10. The majority of numbers fall between 5 and 6. In other words, there are significantly more average wines than extremely good or subpar wines. Because of this, it could be more challenging to define what makes a good or bad wine. To help with this, one may simply divide the wines into two categories: good wine quality and low wine quality.
It would be logical to divide the wines into bad and good wines by 5 or below and 6 or above if one wants to create a binary wine quality variable that is consistent across all wines because 5 and 6 have a pretty even number of wines and have the majority of the wines overall.

```{r}
## Visual Look at the Data Set with Pairs Plot Colored by Good/Bad Wine Quality
ggpairs(wineDfBinaryQuality, 
        aes(alpha=0.6, color = quality),
        upper = list(continuous = wrap("cor", size = 2)),
        diag = list(continuous = "barDiag"),
        lower = list(continuous = "smooth"))
```

Interpretation: One can more clearly distinguish between the two groups when the data is colored by good vs. bad wine. For instance, the alcohol and quality plots in the bottom right corner demonstrate that while some poor wines have low alcohol content (in a skewed distribution), superior wines have a more even distribution and generally have greater alcohol contents. Additionally, one can observe from the scatter plots and trend lines at the bottom left that, for each set of factors, terrible and good wines generally appear to follow a similar trend.
We can observe how each numerical variable is distributed in relation to good vs. bad quality from the box plots for bad vs. good quality on the right. As was already said, fine wines typically have more alcohol. In general, good wines also tend to have more citric acid and sulfates. Bad wines typically have slightly greater densities and more volatile acidity. Based on the box plots, it is difficult to visually distinguish between good and terrible wines using the other numerical factors.

3. Model implementation
```{r}
## Spliting the data into training set and test set
df <- read.csv("winequality-red.csv")
df$quality <- as.factor(df$quality)
set.seed(123)
index <- sample(1:nrow(df),size = 0.8*nrow(df))
train <- df[index,]
test <- df[-index,]
## Linear regression
df_lm <- read.csv("winequality-red.csv")
train_lm <- df_lm[index,]
test_lm <- df_lm[-index,]
lm_model <- glm(quality ~., data = train_lm)
lm_prediction <- predict(lm_model, test_lm)
lm_prediction <- round(lm_prediction, digits = 0)
lm_prediction <- as.factor(lm_prediction)
test_lm$quality <- as.factor(test_lm$quality)
lm_matrix <- confusionMatrix(lm_prediction, test_lm$quality)
lm_matrix

# logistic regression
df_lr <- read.csv("winequality-red.csv")
df_lr$quality <- ifelse(df_lr$quality>5,1,0)
df_lr$quality <- as.factor(df_lr$quality)
train_lr <- df_lr[index,]
test_lr <- df_lr[-index,]
lr_model <- glm(quality ~., data = train_lr, family = "binomial")
lr_prediction <- predict(lr_model, test_lr, type = "response")
lr_prediction <- round(lr_prediction, digits = 0)
lr_prediction <- as.factor(lr_prediction)
lr_matrix <- confusionMatrix(lr_prediction, test_lr$quality)
lr_matrix
```

Interpretation: The linear model’s performance is ordinary, according to the evaluation metrics we use here. Only a 0.6219 accuracy out of 1.0, with CI of (0.5663, 0.6752). The kappa coefficient was calculated as 0.3489, which is a considerably fair result.
The performance of logstic model does improve referring to the evaluation metrics here, compared to linear regression model fitted earlier. (0.7375 accuracy out of 1.0, with CI of (0.6857, 0.7489)) However, it's much easier for the model to guess right when there are only 2 possible outcomes in this case.

```{r}
# simplest random forest
library(randomForest)
rf_model <- randomForest(quality ~., data = train)
rf_prediction <- predict(rf_model, test)
library(caret)
rf_matrix <- confusionMatrix(rf_prediction, test$quality)
rf_matrix

# More complicated random forest using caret and ranger packages (10-fold cross-validation)
library(ranger)
caret_rf_model <- train(
  quality ~ .,
  tuneLength = 10,
  data = train, method = "ranger",
  trControl = trainControl(method = "cv", number = 10, verboseIter = FALSE)
)
caret_rf_predict <- predict(caret_rf_model, test)
caret_rf_matrix <- confusionMatrix(caret_rf_predict, test$quality)
caret_rf_matrix

## look at the importance of variables
varImp(rf_model) %>% kable
varImpPlot(rf_model)
```

Interpretation: The variable importance plot in Figure 8 provides a list of the most significant variables in descending order by a mean decrease in Gini. The top variables contribute more to the model than the bottom ones and also have high predictive power in classifying default and non-default customers.
a) In this case, alcohol has the largest mean decrease in Gini compared to other variables, which demonstrates that it has high predictive power in predicting our target variable – wine quality. This is kind of reasonable since A wine with a higher alcohol content will have a fuller, richer body, while a lower-level alcohol wine will taste lighter and more delicate on the palate (Masterclass Staff, 2020).
b) Sulfites are a group of chemical compounds found naturally in a variety of foods and beverages. Research shows that a small percentage of the population is even sensitive to sulfites and may experience side effects like headaches, hives, swelling, stomach pain, and diarrhea. That may be one of the reasons why it’s the second most important of our all variables.
c) Besides, Total sulfur dioxide, Volatile acidity and Density also show high importance in our analysis, which may contribute to the realistic wine manufacturing process.

```{r}
# SVM
library(e1071)

tune_out <- 
    tune.svm(x = train[, -12], y = train[, 12], 
             type = "C-classification", cost = c(0.1, 1, 10, 100), 
             gamma = c(0.1, 1, 10), coef0 = c(0.1, 1, 10))

svm_model <- svm(quality~ ., data = train, type = "C-classification", 
                 cost = tune_out$best.parameters$cost, 
                 gamma = tune_out$best.parameters$gamma, 
                 coef0 = tune_out$best.parameters$coef0)

svm_predict <- predict(svm_model, test)
svm_predict <- as.factor(svm_predict)
svm_matrix <- confusionMatrix(svm_predict, test$quality)
svm_matrix
```

The accuracy of SVM is 0.6688, which indicates that the SVM model correctly predicts the value for quality more than 66% of the time. It’s just slightly worse than the random forest model, and much better than linear regression.